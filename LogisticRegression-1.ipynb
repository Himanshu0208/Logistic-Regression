{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2acd98d4-8766-479e-bff9-a8f96196b2ee",
   "metadata": {},
   "source": [
    "# __Ques 1__\n",
    "__Linear Regression:__\n",
    "<br>Linear Regression is a statical model which is used to predict a continous dependent variable based on one or more independent variable. It assumes a linear realtionship between dependent and independent variables. <br>\n",
    "EX:- predicting the height of students based on their weights. here the height is a dependent variable and is continous too.\n",
    "<br><br>\n",
    "__LogisticRegression:__\n",
    "<br>Logistic Regression is a statical model which is used to predict the probablility of a categorical outcome based on one or more independent variables. In this we predict the probability of occuring that particular datapoint into all categories and then we put it into the category which has maximum probability\n",
    "<br>EX:- predicting whether to give loan to a particular person based on thier existing loans and credit reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecbd1a3-e207-4553-bdf0-e66873230947",
   "metadata": {},
   "source": [
    "# __Ques 2__\n",
    "in logistic regression we use log loss cost function having a the best fit line as a sigmoid function<br>\n",
    "J(&theta;1 , &theta;2) = -y * log(h&theta;(x)) - (1-y) * log(1-h&theta;(x))<br>\n",
    "where h&theta;(x) = 1 / (1 + e<sup>-(&theta;1 + &theta;2 * x)</sup>)\n",
    "<br><br>\n",
    "the cost function is optimized by graident decent using the convergence algorithm<br>\n",
    "repeat<br>\n",
    "{<br>\n",
    "&theta;1 := &theta;2 + &alpha; * âˆ‡J(&theta;)<br>\n",
    "{<br>\n",
    "where &alpha; = learning rate = 0.05(genrally)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146faf94-e796-4ae2-b181-b15b0410b7d1",
   "metadata": {},
   "source": [
    "# __Ques 3__\n",
    "Regularization helps to prevent overfitting of the data by adding a penality term to the cost function. overfitting genrally occurs when the model fits to closely to the training data but fails to genralize new unseen data<br>\n",
    "The two most common regularization are L1 Regularization(Lasso) and L2 Regularization(Rigid). they both add penalty terms to the cost function resulting in shrinking of the parameter estimates towards zero, reducing model complexity.<br>\n",
    "- L1 :-it adds peneality term of &lambda * &sum; |slope|\n",
    "- L1 :-it adds peneality term of &lambda * &sum; (slope)<sup>2</sup>\n",
    "<br>where &lambda = regularization parameter also known as hyper parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a62d3d8-fdf0-4f2b-8f25-1ccf73bb287e",
   "metadata": {},
   "source": [
    "# __Ques 4__\n",
    "In logistic regression, the ROC (Receiver Operating Characteristic) curve is a plot that helps to evaluate the performance of a binary classifier, which predicts the probability of an outcome belonging to one of two classes.\n",
    "<br>\n",
    "The ROC curve is created by plotting the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings. A threshold is a value above which a sample is classified as positive, and below which it is classified as negative. By varying the threshold, we can calculate the true positive rate and the false positive rate for different levels of sensitivity and specificity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dd319f-8baf-4058-8093-a08f5c2820a1",
   "metadata": {},
   "source": [
    "# __Ques 5__\n",
    "__Feature Selection techniques:__\n",
    "- Recursive Elemination :- this techinique recursively removes the least important feature based coefficients obtained from logistic regression model. the process continues untill a particular no of features reached\n",
    "- L1 Reguralization :-  L1 regularization shrinks some of the coefficients to zero, effectively eliminating the corresponding features from the model. This method is useful when the number of features is very high relative to the number of observations, and it can lead to a more interpretable model. \n",
    "- Principal Component Analysis(PCA) :- PCA is a technique that transforms the original features into a new set of uncorrelated features called principal components. The number of principal components can be reduced based on the amount of variance they explain. This method can reduce the dimensionality of the data and help capture the most important patterns in the data.\n",
    "<br><br>\n",
    "by feature extraction we can imporve our model by reducing the noise , reducing the risk of overfitting , and improve the interpreatibilty of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22504b1d-bc8d-4898-aef5-0310aae8408e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __Ques 6__\n",
    "- Threshold adjustment: By default, the threshold for classification in logistic regression is set to 0.5. However, in the case of imbalanced datasets, it may be more appropriate to adjust the threshold to improve the performance on the minority class. For example, lowering the threshold can increase the sensitivity (true positive rate) of the model on the minority class at the expense of a higher false positive rate.\n",
    "- Resmaplping : We can also resmaple our data either by oversampling the minority class or under sampling the majority class.Oversampling techniques include random oversampling, SMOTE (Synthetic Minority Over-sampling Technique). Undersampling techniques include random undersampling. These techniques help balance the class distribution and improve the model's performance on the minority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80815502-1093-4fe2-8c77-3362f3e8d115",
   "metadata": {},
   "source": [
    "Also if a class is imbalamced then we can not use the normal method for accuracy prediction in such cases we use\n",
    "- precision : when FP (False Poisitive is more important)\n",
    "- Recall : when FN (False Negative is more important)\n",
    "- F-1 Scrore: when both FP & FN are important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba1cf9e-c293-436b-809f-7a09b4fc0419",
   "metadata": {},
   "source": [
    "# __Ques 7__\n",
    "__Problems :__\n",
    "<br>\n",
    "- Multicollinearity: Multicollinearity occurs when there is a high degree of correlation between independent variables in the model, which can cause problems with the stability and interpretability of the coefficients. To address multicollinearity, one approach is to use a technique called ridge regression, which adds a penalty term to the cost function to shrink the coefficients towards zero. Another approach is to remove one of the highly correlated variables from the model.\n",
    "\n",
    "- Overfitting: Overfitting occurs when the model is too complex and fits the training data too closely, leading to poor performance on new data. To address overfitting, one approach is to use regularization techniques, such as Lasso or Ridge regression, which add a penalty term to the cost function to discourage overfitting. Another approach is to use cross-validation to evaluate the model's performance on new data and select the model with the best performance.\n",
    "\n",
    "- Class imbalance: Class imbalance occurs when there are more observations in one class than the other, which can lead to biased models that perform poorly on the minority class. To address class imbalance, one approach is to use resampling techniques, such as oversampling the minority class or undersampling the majority class. Another approach is to use cost-sensitive learning, where different misclassification costs are assigned to each class.\n",
    "\n",
    "- Outliers: Outliers are data points that are significantly different from the rest of the data and can have a large impact on the model's performance. To address outliers, one approach is to use robust regression techniques, such as the Huber loss function or the Tukey bisquare loss function, which are less sensitive to outliers than the standard loss function.\n",
    "\n",
    "- Missing data: Missing data can be a problem when implementing logistic regression, as it can lead to biased models or reduced sample sizes. To address missing data, one approach is to use imputation techniques, such as mean imputation or regression imputation, to estimate the missing values. Another approach is to use models that are robust to missing data, such as maximum likelihood estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0120ec49-46cf-456f-a1c9-3aa49b602c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
