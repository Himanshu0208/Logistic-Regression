{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fead577a-48e4-4456-af77-a3bc245a81f8",
   "metadata": {},
   "source": [
    "# __Ques 1__\n",
    "GridSearchCV is a method which is used for cross validation of the model's of machine learning. It helps in finding the best possible combinations of the features (which are given to it in the form of dictornary where the key are paramneter as the possible values of keys are passed in form tupel). It's working is very simple it just makes all the possible combination of the the parameters passed and run them on the data and then at last fits that resturns the parameters values which makes the best accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e967bb-48d2-4888-9962-eda84ec31317",
   "metadata": {},
   "source": [
    "# __Ques 2__\n",
    "There is only one major difference between the grid search cv and randomized search cv wiz. hat randomized search cv also takes a parameters named as n_iters now instead of making all the combinations it runst randomly makes n_iters combination and find the best parameters between them. \n",
    "<br><br>\n",
    "WE should use randomizd WE should use randomixed search cv when the data point are very large in numeber as at that point earch cv when the data point are very large in numeber as at that point of time the time complexity is very high for grid search cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e466d0-eaf5-443d-aff6-80a9831ae168",
   "metadata": {},
   "source": [
    "# __Ques 3__\n",
    "Data Leakege is a condition in which the model already knows about the test dataset at the time of its fitting as due to this it will fit itself for test data as well hence resulting in the high accuracy on the test data too but when the outer world data comes than the model performs poorly.<br>\n",
    "EX:- If model is fiited using test data to than model will adujuste itself for test data to hence provide wrong accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f874c1-0516-44db-8ead-820b125bbe7a",
   "metadata": {},
   "source": [
    "# __Ques 4__\n",
    "To prevent the data leakage we just have to split the test and train data and then make sure that the seprated test data is not passed to the model for fitting of thee model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e52488-0830-472f-8798-b8f8396a338c",
   "metadata": {},
   "source": [
    "# __Ques 5__\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing the actual class labels of a dataset to the predicted class labels produced by the model. It is also known as an error matrix or a contingency table.\n",
    "<br>\n",
    "A confusion matrix typically has two dimensions: the predicted class labels and the actual class labels. It contains four values: true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN). The values represent the number of instances that were correctly or incorrectly classified by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4d75cf-83a5-4f92-b24a-d436fc890d0b",
   "metadata": {},
   "source": [
    "# __Ques 6__\n",
    "- Precision : when FP (False Poisitive is more important) formula = TP / (TP + FP). It shows proportion of true positive in all positive predictions .The high precision tells that the model is making few false positive errors.\n",
    "\n",
    "- Recall : when FN (False Negative is more important). Formula = TP / (TP + FN). It shows the proportion of true positvie in all actual positive instance. The high recall tells that the model is making few false negative errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2888b550-677f-4c8d-99cd-7f2cbf5e57c8",
   "metadata": {},
   "source": [
    "# __Ques 7__\n",
    "- Accuracy :- the accuracy of the model is calculated as = (TP + TN) / (TP + TN + FP + FN). it is usefull in calcualting the proportion of correctly classified datapoints by total datapoints. a high accuracy means that the model is performing well overall\n",
    "\n",
    "- Precision : when FP (False Poisitive is more important) formula = TP / (TP + FP). It shows proportion of true positive in all positive predictions .The high precision tells that the model is making few false positive errors.\n",
    "\n",
    "- Recall : when FN (False Negative is more important). Formula = TP / (TP + FN). It shows the proportion of true positvie in all actual positive instance. The high recall tells that the model is making few false negative errors.\n",
    "\n",
    "- F-1 Score : the f1 score is defined as (2*(presion * recall))/ (presion + recall), which represent weighted average of presion and recall. High f1 score  means that the model is preforming well on the precision as well as the recall\n",
    "\n",
    "- False Positive rate (FPR) :- FP / ( TP + FP ), it represents the the proportion of negative instance that are incorrectly classified as positive instance. High FPR means the model is making Large False Positive errors\n",
    "\n",
    "- False Negative rate (FNR) :- FN / ( TP + FN ), it represents the the proportion of positve instance that are incorrectly classified as neative instance. High FPR means the model is making Large False Negative errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28d1ed-a6b8-48a7-a4bf-5a2667376897",
   "metadata": {},
   "source": [
    "# __Ques 8__\n",
    "There are several common metrics that can be derived from a confusion matrix, and each provides different insights into the performance of a classification model. Here are some of the most commonly used metrics:\n",
    "\n",
    "- Accuracy: Accuracy is the proportion of correctly classified instances out of the total number of instances in the dataset. It is calculated as (TP + TN) / (TP + FP + TN + FN).\n",
    "\n",
    "- Precision: Precision is the proportion of true positive predictions out of all positive predictions. It is calculated as TP / (TP + FP).\n",
    "\n",
    "- Recall: Recall is the proportion of true positive predictions out of all actual positive instances in the dataset. It is calculated as TP / (TP + FN).\n",
    "\n",
    "- F1-score: F1-score is the harmonic mean of precision and recall. It is a weighted average of precision and recall, with higher values indicating better performance. It is calculated as 2 * (precision * recall) / (precision + recall).\n",
    "\n",
    "- Specificity: Specificity is the proportion of true negative predictions out of all actual negative instances in the dataset. It is calculated as TN / (TN + FP).\n",
    "\n",
    "- False positive rate: False positive rate is the proportion of false positive predictions out of all actual negative instances in the dataset. It is calculated as FP / (TN + FP).\n",
    "\n",
    "- False negative rate: False negative rate is the proportion of false negative predictions out of all actual positive instances in the dataset. It is calculated as FN / (TP + FN)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b53696b-f04c-43fb-8d42-eb4e38db1afb",
   "metadata": {},
   "source": [
    "# __Ques 9__\n",
    "The accuracy of the model is itself derived from the confusio matrix. as there are 4 parts of confusion matrix wiz., TP(True Positive) , TN(True Negative), FN(False Negative), FP(False Positive) and the accuracy of the model is nothing but <br>\n",
    "Accuracy = (TP + TN)/(TP + TN + FP + FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482685f6-a6b3-41d8-84d8-1a3b974e3306",
   "metadata": {},
   "source": [
    "# __Ques 10__\n",
    "- Class imbalance: If there is a significant difference in the number of instances in each class, the model may be biased towards the majority class. A confusion matrix can reveal this by showing a large number of false negatives or false positives in the minority class, indicating that the model is not able to correctly classify instances in that class.\n",
    "- Overfitting: If the model is overfitting to the training data, it may perform well on the training set but poorly on new, unseen data. This can be detected by comparing the performance of the model on the training and validation/test data. If there is a significant difference in performance, the model may be overfitting. The confusion matrix can reveal which classes or categories are particularly affected by overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c747a29-db6f-40d7-be8d-e88ff4246237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
